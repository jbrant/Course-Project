\relax 
\citation{shapiro2001computer}
\citation{morris2004computer}
\citation{sonka2008image}
\citation{DBLP:journals/corr/RussakovskyDSKSMHKKBBF14}
\citation{bourland1988}
\citation{baldi2012autoencoders}
\citation{stanfordimage}
\citation{stanfordimage}
\citation{stanfordimage}
\citation{Stanley:2002:ENN:638553.638554}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Autoencoders}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces An autoencoder with 6 visible units and 3 hidden units. The input is encoded from Layer L1 to Layer L2, and decoded from Layer L2 to Layer L3 \cite  {stanfordimage}.\relax }}{\thepage }}
\citation{Stanley:2009:HEE:1516090.1516093}
\citation{Stanley:2007:CPP:1265496.1265517}
\citation{Stanley:2009:HEE:1516090.1516093}
\citation{Stanley:2009:HEE:1516090.1516093}
\citation{Stanley:2009:HEE:1516090.1516093}
\citation{bourland1988}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}NEAT}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}HyperNEAT}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The "state-space sandiwch" substrate - ideal for visual mapping \cite  {Stanley:2009:HEE:1516090.1516093}.\relax }}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {3}Approach}{\thepage }}
\citation{mnistdataset}
\bibstyle{ieeetran}
\bibdata{MidtermReport}
\bibcite{shapiro2001computer}{1}
\bibcite{morris2004computer}{2}
\bibcite{sonka2008image}{3}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The "state-space sandwich" substrate used in the evolved autoencoder experiments. The hidden layer may vary depending on the experiment configuration, but the general three-tiered structure will remain.\relax }}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The minimal, starting-state (no hidden nodes) CPPN. The inputs to the CPPN are the X and Y coordinates on the source substrate and the X and Y coordinates on the target substrate. Depending on which layer the CPPN is querying, it will either output the weight of the connection between the input substrate and hidden substrate along with the weight of the bias connection between the same, or the weight of the connection between the hidden substrate and output substrate along with the weight of the bias connection between the same.\relax }}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion of Current State}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}NEAT Results}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}HyperNEAT Results}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Plan for Completion}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {5}References}{\thepage }}
\bibcite{DBLP:journals/corr/RussakovskyDSKSMHKKBBF14}{4}
\bibcite{bourland1988}{5}
\bibcite{baldi2012autoencoders}{6}
\bibcite{stanfordimage}{7}
\bibcite{Stanley:2002:ENN:638553.638554}{8}
\bibcite{Stanley:2009:HEE:1516090.1516093}{9}
\bibcite{Stanley:2007:CPP:1265496.1265517}{10}
\bibcite{mnistdataset}{11}
