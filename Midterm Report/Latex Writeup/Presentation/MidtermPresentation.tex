\documentclass[xcolor=dvipsnames]{beamer}

\graphicspath{ {../Includes/} }

\usepackage[export]{adjustbox}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage[labelfont=bf]{caption}
\usepackage{array}

\usetheme{Copenhagen}

\newcommand\Fontvi{\fontsize{7}{10}\selectfont}

\title{A Farewell to Structural Rigidity}
\subtitle{Evolving the Starting State of Autoencoders}
\author[Ross \and Brant \and Roessler]{Christopher Ross \and Jonathan Brant \and Zak Roessler}

\begin{document}

	\begin{frame}
		\titlepage
	\end{frame}
	
	\section{Introduction}
	\begin{frame}{Introduction}	
		\begin{itemize}
			\item Image Processing/Classification
			\begin{enumerate}
				\item Each pixel is a separate input
				\item ANN inputs grows exponentially as resolution increases
				\item Method of dimensionality reduction needed
			\end{enumerate}			
			\item Autoencoders learns compressed representation
			
			\item Traditionally, autoencoder hidden layer topology has been hand-crafted
			
			\item Our approach evolves autoencoder structure that can then be optimized			
			\begin{enumerate}
				\item Hidden layer of autoencoders evolved (using HyperNEAT)
				\item Backpropagation optimizes evolved autoencoder
			\end{enumerate}
		\end{itemize}
	\end{frame}
	
	\section{Background}	
	\begin{frame}{Autoencoders}	
		\begin{columns}[T]
			\begin{column}{.5\textwidth}
				\begin{itemize}
					\item Autoencoders attempt to extract most salient features from visible layer	
					\item Typical layers include:
					\begin{enumerate}\Fontvi
						\item Input (visible) layer where uncompressed features are received
						\item Hidden layer where compressed features are stored
						\item Output layer where reconstruction error is calculated
					\end{enumerate}
					\item Most often trained using backpropagation with the intent of minimizing reconstruction error
				\end{itemize}
			\end{column}
	
			\begin{column}{.5\textwidth}
				\includegraphics[width=\textwidth]{ExampleAutoencoder}
			\end{column}
		\end{columns}
	\end{frame}
	
	\begin{frame}{HyperNEAT}
		\begin{columns}[T]
			\begin{column}{.5\textwidth}
				\begin{itemize}
					\item Indirect encoding of HyperNEAT through CPPNs substantially reduces search space
					\begin{enumerate}\Fontvi
						\item We're no longer looking at each pixel individually
					\end{enumerate}
					\item Substrate configuration hand-designed to capture domain geometry
					\begin{enumerate}\Fontvi
						\item State-space sandwich substrate ideal for visual mapping
					\end{enumerate}
					\item Ability to scale with little-to-no loss of function ideal for image processing at varying resolutions
				\end{itemize}
			\end{column}
			\begin{column}{.5\textwidth}
				\includegraphics[width=\textwidth]{StateSpaceSandwichSubstrate}
			\end{column}
		\end{columns}			
	\end{frame}
	
	\section{Approach}
	\begin{frame}{Evolution of Autoencoders}
		\begin{itemize}
			\item Two-phase process:
			\begin{enumerate}
				\item HyperNEAT evolves initial autoencoder
				\item Backpropagation fine tunes autoencoder weights
			\end{enumerate}
			
			\item Results will be analyzed:
			\begin{enumerate}
				\item Quantitatively - reduction of reconstruction error
				\item Qualitatively - how closely does reconstructed image resemble the original
			\end{enumerate}

			\item MNIST handwritten digit dataset will be used for training/validation			
		\end{itemize}
		\begin{figure}[h]
			\centering
			\includegraphics[width=\textwidth,height=0.1\textheight,keepaspectratio]{MNIST_0s}
			\includegraphics[width=\textwidth,height=0.1\textheight,keepaspectratio]{MNIST_9s}
		\end{figure}
	\end{frame}
	
	\begin{frame}{Substrate Configuration}
		\begin{columns}[T]
			\begin{column}{.5\textwidth}
				\begin{itemize}
					\item Three-tiered state-space sandwich substrate
					\begin{enumerate}
						\item Input sheet - all points in 2-dimensional image (size varies based on image resolution)
						\item Hidden sheet - compressed feature vector
						\item Output sheet - reconstructed image at original resolution
					\end{enumerate}
				\end{itemize}
			\end{column}
			\begin{column}{.5\textwidth}
				\includegraphics[width=\textwidth,height=0.8\textheight,keepaspectratio]{SubstrateConfiguration/AutoencoderSubstrate}
			\end{column}
		\end{columns}	
	\end{frame}
	
	\begin{frame}{Minimal CPPN}
		\begin{itemize}
			\item CPPN inputs are cartesian coordinates between source and target sheet
			\begin{itemize}\Fontvi
				\item $X_1$ and $Y_1$ is point on source sheet
				\item $X_2$ and $Y_2$ is point on target sheet
			\end{itemize}
			\item CPPN queries the substrate twice:
			\begin{itemize}\Fontvi
				\item Once for connection weight and bias weight between input and hidden sheet
				\item Once for connection weight and bias weight between hidden and output sheet
			\end{itemize}
		\end{itemize}
		\begin{center}
			\includegraphics[width=\textwidth,height=0.4\textheight,keepaspectratio]{MinimalCppn/MinimalCppn}
		\end{center}
	\end{frame}
	
	\begin{frame}{Experiment Configuration}
		\begin{table}[b]
			\small
			\centering
			\begin{tabular}{|p{4cm}|p{0.8cm}|p{5cm}|}
				\hline\hline
				\textbf{Parameter} & \textbf{Value} & \textbf{Description} \\
		
				\hline
		
				Training Sample Proportion & 80\% & Percentage of sample images used for training \\
				\hline
				Number of Backpropagation Iterations & 1 & Number of backpropagation iterations performed during training \\
				\hline
				Learning Rate & 0.01 & Learning Rate (to control backpropagation convergence speed) \\
				\hline
				Image Resolution Reduction Factor & 2 & Reduction factor of source image (i.e. a 28x28 image becomes 14x14 with a reduction factor of 2) \\
				\hline
				MNIST Digit & 0, 1 & MNIST handwritten digit dataset image used \\
		
				\hline
			\end{tabular}
		\end{table}
	\end{frame}
	
	\section{Discussion}
	\begin{frame}{Current State}
		\begin{itemize}
			\item Standard NEAT initially attempted
			\begin{itemize}
				\item Slow evolution due to massive search space
				\item Qualitatively poor image reconstruction
				\item Reconstruction reached 90\% accuracy, but only because most pixels were black
			\end{itemize}			
		\end{itemize}
		\begin{figure}[h]
			\centering
			\includegraphics[width=\textwidth,height=0.35\textheight,keepaspectratio]{NeatStartingState}
			\hfill
			\includegraphics[width=\textwidth,height=0.35\textheight,keepaspectratio]{NeatEvolvedState}
		\end{figure}
	\end{frame}
	
	\begin{frame}{Current State - HyperNEAT}
		\begin{itemize}
			\item HyperNEAT achieved similar reconstruction accuracy, but qualitatively superior	
			\item Attained over 90\% accuracy in a couple of generations
		\end{itemize}
		\begin{figure}[h]
			\centering
			\includegraphics[scale=0.16]{MinimallyConnectedCPPN}
			\hfill
			\includegraphics[scale=0.13]{FitnessOverGenerations}
		\end{figure}
	\end{frame}

	\begin{frame}{Current State - HyperNEAT}
		\begin{table}[h]
			\small
			\centering
			\begin{tabular}{>{\centering}m{1.7cm} >{\centering}m{2.1cm} >{\centering}m{1.7cm} >{\centering\arraybackslash}m{1.5cm}}
				\hline\hline
				\textbf{Source} & \textbf{Reconstructed} & \textbf{Set} & \textbf{Index} \\
				\hline
				\includegraphics[scale=0.3]{One_Source_Training_Index30} & \includegraphics[scale=0.3]{One_Reconstructed_Training_Index30} & Training & 30 \\
				\includegraphics[scale=0.3]{One_Source_Training_Index60} & \includegraphics[scale=0.3]{One_Reconstructed_Training_Index60} & Training & 60 \\
				\includegraphics[scale=0.3]{One_Source_Validation_Index85} & \includegraphics[scale=0.3]{One_Reconstructed_Validation_Index85} & Validation & 85 \\
				\includegraphics[scale=0.3]{One_Source_Validation_Index95} & 		\includegraphics[scale=0.3]{One_Reconstructed_Validation_Index95} & Validation & 95 \\
			\end{tabular}
		\end{table}
	\end{frame}
\end{document}